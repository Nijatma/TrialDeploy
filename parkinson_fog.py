# -*- coding: utf-8 -*-
"""Parkinson_FOG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aPOf8JzwYDM66NHnNEPXVRcLgTizSxE7

Load Data : original format text file ----> convert to csv format giving appropriate column names for the features as described by authors of the Daphnet Dataset
"""

import pandas as pd
import numpy as np
from scipy import signal

from google.colab import drive
drive.mount('/content/drive')

# Define column names
column_names = [
    "Time", "Ankle_horizontal_forward", "Ankle_vertical", "Ankle_horizontal_lateral",
    "Upper_leg_horizontal_forward", "Upper_leg_vertical", "Upper_leg_horizontal_lateral",
    "Trunk_horizontal_forward", "Trunk_vertical", "Trunk_horizontal_lateral","Annotation"
]

# Read the text file into a DataFrame
df1 = pd.read_csv("/content/drive/MyDrive/dataset/S01R01.txt", delimiter=" ", names=column_names)
df2 = pd.read_csv("/content/drive/MyDrive/dataset/S01R02.txt", delimiter=" ", names=column_names)
df3 = pd.read_csv("/content/drive/MyDrive/dataset/S02R01.txt", delimiter=" ", names=column_names)
df4 = pd.read_csv("/content/drive/MyDrive/dataset/S02R02.txt", delimiter=" ", names=column_names)
df5 = pd.read_csv("/content/drive/MyDrive/dataset/S03R01.txt", delimiter=" ", names=column_names)
df6 = pd.read_csv("/content/drive/MyDrive/dataset/S03R02.txt", delimiter=" ", names=column_names)
df7 = pd.read_csv("/content/drive/MyDrive/dataset/S03R03.txt", delimiter=" ", names=column_names)
df8 = pd.read_csv("/content/drive/MyDrive/dataset/S04R01.txt", delimiter=" ", names=column_names)
df9 = pd.read_csv("/content/drive/MyDrive/dataset/S05R01.txt", delimiter=" ", names=column_names)
df10 = pd.read_csv("/content/drive/MyDrive/dataset/S05R02.txt", delimiter=" ", names=column_names)
df11 = pd.read_csv("/content/drive/MyDrive/dataset/S06R01.txt", delimiter=" ", names=column_names)
df12 = pd.read_csv("/content/drive/MyDrive/dataset/S06R02.txt", delimiter=" ", names=column_names)
df13 = pd.read_csv("/content/drive/MyDrive/dataset/S07R01.txt", delimiter=" ", names=column_names)
df14 = pd.read_csv("/content/drive/MyDrive/dataset/S07R02.txt", delimiter=" ", names=column_names)
df15 = pd.read_csv("/content/drive/MyDrive/dataset/S08R01.txt", delimiter=" ", names=column_names)
df16 = pd.read_csv("/content/drive/MyDrive/dataset/S09R01.txt", delimiter=" ", names=column_names)
df17 = pd.read_csv("/content/drive/MyDrive/dataset/S10R01.txt", delimiter=" ", names=column_names)

# Create a list to hold your dataframes
dataframes = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11, df12, df13, df14, df15, df16, df17]

# Concatenate dataframes vertically
data = pd.concat(dataframes, ignore_index=True)

# Print the first few rows of the combined dataframe
data.head(10)

data.shape

"""First feature - acceleration magnitude"""

acc_magnitude = np.sqrt(data['Ankle_horizontal_forward']**2 + data['Ankle_vertical']**2 + data['Ankle_horizontal_lateral']**2)

type(acc_magnitude)

acc_magnitude.shape

data['Acc_magnitude'] = acc_magnitude

data.shape

len(data)

"""Next set of features (for Ankle Sensor) :

1. Mean
2. Standard Deviation
3. Variance
"""

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Ankle_horizontal_forward'].mean())
    variance_values.append(window_data['Ankle_horizontal_forward'].var())
    std_deviation_values.append(window_data['Ankle_horizontal_forward'].std())

""" Add the calculated features to the DataFrame
data['Windowed_Mean'] = mean_values
data['Windowed_Variance'] = variance_values
data['Windowed_Std_Deviation'] = std_deviation_values"""
len(mean_values)
len(variance_values)
len(std_deviation_values)

print(mean_values[len(mean_values)-1])
print(variance_values[len(variance_values)-1])
print(std_deviation_values[len(std_deviation_values)-1])

len(data)-len(mean_values)

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))

list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))

list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))

mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)

len(mean_values)
len(variance_values)
len(std_deviation_values)

""" Add the calculated features to the DataFrame"""
data['Ankle_horizontal_forward_Windowed_Mean'] = mean_values
data['Ankle_horizontal_forward_Windowed_Variance'] = variance_values
data['Ankle_horizontal_forward_Windowed_Std_Deviation'] = std_deviation_values

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Ankle_vertical'].mean())
    variance_values.append(window_data['Ankle_vertical'].var())
    std_deviation_values.append(window_data['Ankle_vertical'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Ankle_vertical_Windowed_Mean'] = mean_values
data['Ankle_vertical_Windowed_Variance'] = variance_values
data['Ankle_vertical_Windowed_Std_Deviation'] = std_deviation_values

data.shape

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Ankle_horizontal_lateral'].mean())
    variance_values.append(window_data['Ankle_horizontal_lateral'].var())
    std_deviation_values.append(window_data['Ankle_horizontal_lateral'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Ankle_horizontal_lateral_Windowed_Mean'] = mean_values
data['Ankle_horizontal_lateral_Windowed_Variance'] = variance_values
data['Ankle_horizontal_lateral_Windowed_Std_Deviation'] = std_deviation_values

data.shape

"""Next set of features (for Upper Leg Sensor) :

1. Mean
2. Standard Deviation
3. Variance
"""

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Upper_leg_horizontal_forward'].mean())
    variance_values.append(window_data['Upper_leg_horizontal_forward'].var())
    std_deviation_values.append(window_data['Upper_leg_horizontal_forward'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Upper_leg_horizontal_forward_Windowed_Mean'] = mean_values
data['Upper_leg_horizontal_forward_Windowed_Variance'] = variance_values
data['Upper_leg_horizontal_forward_Windowed_Std_Deviation'] = std_deviation_values

data.shape

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Upper_leg_vertical'].mean())
    variance_values.append(window_data['Upper_leg_vertical'].var())
    std_deviation_values.append(window_data['Upper_leg_vertical'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Upper_leg_vertical_Windowed_Mean'] = mean_values
data['Upper_leg_vertical_Windowed_Variance'] = variance_values
data['Upper_leg_vertical_Windowed_Std_Deviation'] = std_deviation_values

data.shape

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Upper_leg_horizontal_lateral'].mean())
    variance_values.append(window_data['Upper_leg_horizontal_lateral'].var())
    std_deviation_values.append(window_data['Upper_leg_horizontal_lateral'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Upper_leg_horizontal_lateral_Windowed_Mean'] = mean_values
data['Upper_leg_horizontal_lateral_Windowed_Variance'] = variance_values
data['Upper_leg_horizontal_lateral_Windowed_Std_Deviation'] = std_deviation_values

data.shape

"""Next set of features (for Trunk Sensor) :

1. Mean
2. Standard Deviation
3. Variance
"""

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Trunk_horizontal_forward'].mean())
    variance_values.append(window_data['Trunk_horizontal_forward'].var())
    std_deviation_values.append(window_data['Trunk_horizontal_forward'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Trunk_horizontal_forward_Windowed_Mean'] = mean_values
data['Trunk_horizontal_forward_Windowed_Variance'] = variance_values
data['Trunk_horizontal_forward_Windowed_Std_Deviation'] = std_deviation_values

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Trunk_vertical'].mean())
    variance_values.append(window_data['Trunk_vertical'].var())
    std_deviation_values.append(window_data['Trunk_vertical'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Trunk_vertical_Windowed_Mean'] = mean_values
data['Trunk_vertical_Windowed_Variance'] = variance_values
data['Trunk_vertical_Windowed_Std_Deviation'] = std_deviation_values

window_size = 100  # Choose an appropriate window size

# Initialize empty arrays to store calculated features
mean_values = []
variance_values = []
std_deviation_values = []

# Iterate through the data using the sliding window
for i in range(0, len(data) - window_size + 1, 1):
    window_data = data.iloc[i:i+window_size]

    # Calculate mean, variance, and standard deviation for the window
    mean_values.append(window_data['Trunk_horizontal_lateral'].mean())
    variance_values.append(window_data['Trunk_horizontal_lateral'].var())
    std_deviation_values.append(window_data['Trunk_horizontal_lateral'].std())

list1=[mean_values[len(mean_values)-1]]*(len(data)-len(mean_values))
list2=[variance_values[len(variance_values)-1]]*(len(data)-len(mean_values))
list3=[std_deviation_values[len(std_deviation_values)-1]]*(len(data)-len(mean_values))
mean_values.extend(list1)
variance_values.extend(list2)
std_deviation_values.extend(list3)
""" Add the calculated features to the DataFrame"""
data['Trunk_horizontal_lateral_Windowed_Mean'] = mean_values
data['Trunk_horizontal_lateral_Windowed_Variance'] = variance_values
data['Trunk_horizontal_lateral_Windowed_Std_Deviation'] = std_deviation_values

data.shape

"""# Export DataFrame to CSV
csv_filename = 'my_dataframe.csv'

data.to_csv(csv_filename, index=False)

from google.colab import files

# Download the CSV file to local machine
files.download(csv_filename)
"""

# Save DataFrame as CSV on Google Drive
drive_path = '/content/drive/MyDrive/'  # Path to your Google Drive root
csv_filename = 'parkinson_fog_dataframe_drive.csv'
data.to_csv(drive_path + csv_filename, index=False)

dataframe_from_drive = pd.read_csv("/content/drive/MyDrive/parkinson_fog_dataframe_drive.csv")
print(dataframe_from_drive.shape)
## Read CSV file into DataFrame
#df = pd.read_csv(file_path)

dataframe_from_drive['Ankle_horizontal_lateral_Windowed_Variance'].head(5)

acc_magnitude_upper_leg = np.sqrt(dataframe_from_drive['Upper_leg_horizontal_forward']**2 + dataframe_from_drive['Upper_leg_vertical']**2 + dataframe_from_drive['Upper_leg_horizontal_lateral']**2)

acc_magnitude_upper_leg.shape

acc_magnitude_upper_leg[1100:1110]

dataframe_from_drive["Acc_magnitude_upper_leg"]=acc_magnitude_upper_leg

dataframe_from_drive.shape

acc_magnitude_trunk = np.sqrt(dataframe_from_drive['Trunk_horizontal_forward']**2 + dataframe_from_drive['Trunk_vertical']**2 + dataframe_from_drive['Trunk_horizontal_lateral']**2)

dataframe_from_drive["Acc_magnitude_trunk"]=acc_magnitude_trunk

dataframe_from_drive.shape

len(dataframe_from_drive)

dataframe_from_drive['Ankle_horizontal_forward'][0]

dataframe_from_drive.head(1)

# Count occurrences where the value in the specified column is zero
count_zeros = (dataframe_from_drive['Acc_magnitude'] == 0).sum()
print(count_zeros)

# Filter the DataFrame based on the condition and print 'Annotation' values
filtered_annotations = dataframe_from_drive.loc[dataframe_from_drive['Acc_magnitude'] == 0, 'Annotation']

print("Annotation values where Acc_magnitude is zero:")
print(filtered_annotations)

"""WE CAN SAFELY DROP THE ROWS WHERE ANKLE ACCELERATION MAGNITUDE IS ZERO
AS IT IS A PART OF DEBREIFING OF THE PATIENT I.E CLASS 0 (WHICH IS NOT STRICTLY A PART OF THE EXPERIMENT)
"""

# Drop rows where 'Acc_magnitude' is zero
dataframe_from_drive = dataframe_from_drive[dataframe_from_drive['Acc_magnitude'] != 0]

# Reset the index to consecutive values
dataframe_from_drive = dataframe_from_drive.reset_index(drop=True)

# Count occurrences where the value in the specified column is zero
count_zeros = (dataframe_from_drive['Acc_magnitude'] == 0).sum()
print(count_zeros)

count_zeros = (dataframe_from_drive['Acc_magnitude_upper_leg'] == 0).sum()
print(count_zeros)

count_zeros = (dataframe_from_drive['Acc_magnitude_trunk'] == 0).sum()
print(count_zeros)

"""Next feature : angle between two consecutive acceleration vectors (to see how acceleration vector is turning)
cosine theta = dot product of the acceleration vectors at i and i+1 time/ magnitude of acceleration at i * magnitude of acceleration at i+1
"""

import math
list_ankle_theta=[]
list_upper_leg_theta=[]
list_trunk_theta=[]
for i in range(0,len(dataframe_from_drive)-1):
#for i in range(0,100):
  x=((dataframe_from_drive['Ankle_horizontal_forward'][i]*dataframe_from_drive['Ankle_horizontal_forward'][i+1])+(dataframe_from_drive['Ankle_vertical'][i]*dataframe_from_drive['Ankle_vertical'][i+1])+(dataframe_from_drive['Ankle_horizontal_lateral'][i]*dataframe_from_drive['Ankle_horizontal_lateral'][i+1]))/(dataframe_from_drive['Acc_magnitude'][i]*dataframe_from_drive['Acc_magnitude'][i+1])
  #now x represents cosine theta so taking inverse of that to get theta
  #print(x)
  if(x>1.0):
    x=1.0
  if(x<-1.0):
    x=-1.0
  theta= math.acos(x)
  #insert this in dataframe -> represents angle between i and i+1 acceleration vector
  list_ankle_theta.append(theta)
"""
  x1=((dataframe_from_drive['Upper_leg_horizontal_forward'][i]*dataframe_from_drive['Upper_leg_horizontal_forward'][i+1])+(dataframe_from_drive['Upper_leg_vertical'][i]*dataframe_from_drive['Upper_leg_vertical'][i+1])+(dataframe_from_drive['Upper_leg_horizontal_lateral'][i]*dataframe_from_drive['Upper_leg_horizontal_lateral'][i+1]))/(dataframe_from_drive['Acc_magnitude_upper_leg'][i]*dataframe_from_drive['Acc_magnitude_upper_leg'][i+1])
  #now x1 represents cosine theta so taking inverse of that to get theta
  if(x1>1.0):
    x1=1.0
  if(x1<-1.0):
    x1=-1.0
  theta1= math.acos(x1)
  #insert this in dataframe -> represents angle between i and i+1 acceleration vector
  list_upper_leg_theta.append(theta1)
  x2=((dataframe_from_drive['Trunk_horizontal_forward'][i]*dataframe_from_drive['Trunk_horizontal_forward'][i+1])+(dataframe_from_drive['Trunk_vertical'][i]*dataframe_from_drive['Trunk_vertical'][i+1])+(dataframe_from_drive['Trunk_horizontal_lateral'][i]*dataframe_from_drive['Trunk_horizontal_lateral'][i+1]))/(dataframe_from_drive['Acc_magnitude_trunk'][i]*dataframe_from_drive['Acc_magnitude_trunk'][i+1])
  #now x2 represents cosine theta so taking inverse of that to get theta
  if(x2>1.0):
    x2=1.0
  if(x2<-1.0):
    x2=-1.0
  theta2= math.acos(x2)
  #insert this in dataframe -> represents angle between i and i+1 acceleration vector
  list_trunk_theta.append(theta2)
"""

list_ankle_theta[1917870:1917877]

len(list_ankle_theta)

len(dataframe_from_drive)

"""Drop the last row and add the series/list_ankle_theta to the dataframe"""

# Drop the last row
dataframe_from_drive = dataframe_from_drive.drop(dataframe_from_drive.index[-1])

len(dataframe_from_drive)

dataframe_from_drive['Ankle_Vector_Angle']=list_ankle_theta

dataframe_from_drive.shape

# Save DataFrame as CSV on Google Drive
drive_path = '/content/drive/MyDrive/'  # Path to your Google Drive root
csv_filename = 'parkinson_fog_dataframe_saturday26august.csv'
dataframe_from_drive.to_csv(drive_path + csv_filename, index=False)

"""# Additional features to be explored
acc_magnitude = np.sqrt(data['Ankle_horizontal_forward']**2 + data['Ankle_vertical']**2 + data['Ankle_horizontal_lateral']**2)
mean_acc_magnitude = acc_magnitude.mean()
std_acc_magnitude = acc_magnitude.std()
var_acc_magnitude = acc_magnitude.var()

# Apply Fast Fourier Transform (FFT) for frequency domain analysis
frequencies, fft_values = signal.periodogram(data['Ankle_horizontal_forward'], fs=1000)  # Assuming 1000 Hz sampling rate
dominant_frequency = frequencies[np.argmax(fft_values)]
spectral_energy = np.sum(fft_values)

peaks, _ = signal.find_peaks(data['Ankle_horizontal_forward'], distance=50)  # Adjust distance as needed
num_peaks = len(peaks)

step_lengths = np.diff(peaks)
stride_times = step_lengths / 1000.0  # Convert to seconds (assuming 1000 Hz sampling rate)

zero_crossings = np.where(np.diff(np.sign(data['Ankle_horizontal_forward'])))[0]
zero_crossing_rate = len(zero_crossings) / (len(data) / 1000.0)  # Convert to Hz

skewness = ((data['Ankle_horizontal_forward'] - mean_acc_magnitude)**3).mean() / std_acc_magnitude**3
kurtosis = ((data['Ankle_horizontal_forward'] - mean_acc_magnitude)**4).mean() / std_acc_magnitude**4

# Combine extracted features into a feature matrix X
X = np.column_stack([mean_acc_magnitude, std_acc_magnitude, var_acc_magnitude,
                     dominant_frequency, spectral_energy, num_peaks,
                     np.mean(stride_times), np.std(stride_times),
                     zero_crossing_rate, skewness, kurtosis])

# Target labels
y = data['Annotation']
"""